RIGHT NOW:

- pull benchmarks into benches folder (best practices ;) )
- extract magic variables to config (maybe cargo.toml or just pass in settings map)
- pull lsmtree into its own class to eliminate remaining coupling and "prop drilling"
- implement iterator (maybe plan out optimized version first and then implement optimized version)

TODO:

TODO LATER (NEEDS PRIORITIZATION):
- implement either mvcc or serializable snapshot isolation for transactions

- to implement various consistency guarantees for a distributed version, reads MUST be monotonic (some writes executing for the same key K: X1, X2. X1 must not be read if X2 was written later). care needs to be taken with the skiplist data structure wrt. lock-free and wait-free syncronization. it needs to respect that ordering. likewise for the WAL log (this needs to be append only anyway, so I'm not sure if I could ever avoid some form of totally-mutual syncronization here (this is the channel right now)). can this happen? I think it can, even if X1 resolves first, X2 can return to the user sooner. How to implement KV store as multithreaded and still allow for distributed consistency guarantees- without external synchronization affecting performance too much?
- add ycsb benchmarks (there is a repo for this, but it needs to be extended to make it work with this (this repo's support for adding new engines is in progress/stale, need to reach out to them)) 
- verify (prove) db durability + implement FS level manifest WAL to handle multiple/non atomic FS operations (varies OS to OS, everything should be atomic if POSIX compliant)
- eliminate needless copies (is this done?)
- implement custom merge operator
- extend errors so they are more actionable (most mean something went horribly wrong)
- implement multithreaded compaction (send read IOs at once if possible, compact in background)
- write fuzzer (should be easier and can base off of load tests)
- abstract away compaction logic (for potentially more compactions)
- rate limit compaction (static, additive/multiplicative increase, multiplicative decrease)
- support tables (tuples for values)
- compression/checksums (GZIP or similar) (need to think of how this affects flushing a block)
- allow user to customize compression/checksums
- more stuff for dbs ((p)merkle tree differences for anti-entropy mechanisms, configuring WAL log so it can hook up to replication logs like Raft(right now serde is abstracted, so all that needs to be done is wrap so user can choose where writes go))
- more verification (both just in time and defensive checks) (JIT: read what you write, def: check blocks/checksums on startup and every some epoch (what is reasonable? 8 hours? every X MB? should this be affected by time/disk consumption? this should be limited in throughput like compaction)
- monitoring + metrics  (rocksdb calls these stats) (see below)
- multiget/multiput (see rocksdb)
- error correction (RAID support, ECC, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1233620/ -> DNA does not have SIMPLE error correcting codes)
- compile db settings (which algorithms they choose, persistence settings). 
verify this on startup so user cannot corrupt db from providing mismatching settings (usually this would be some migration) note: this too can get corrupted. maybe this should be stored in its own table? maybe that is why sql dbms typically store their data in their own tabels
- migrations (avoid as long as possible- we do not have active users)

- Avoid writing tombstone when compacting
  we can surely do this when compacting the current top level
 however, how can we do this if its not the current top level?
 could use bloom filter...however the ~97% chance tp tn exponentially decreases
 even by 20 tables, this is still better than guessing (~54%)
 so if we cache the bloom filters on startup, this is an option.
- Try reducing most blocks below 1 block size (so its only 1 r/w IO rather than 2)
metrics to implement:
- wal replays
- wal replay total bytes
- num sstables
- disk usage
- write buffer queue length
- avg/median sstable size
- overhead (lsm size) / (number of all bytes of keys, values, not including lengths)
- read ios / read operation
- write ios / write operation
- corruption
- filter false positive rate
- cache hit rate
- table iterations (iterators)
- lock contention times (for r/w, for memtable/fs)
- lock-free/wait-free mechanism contention times (if implemented)
- io time
- compression time
- wal log flushes, ratio of wal log flushes that are duration or accumulated
- time between wal flushes
- number of current immutable memtables